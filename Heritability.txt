Step 1: LD Score Calculation
# Calculate LD scores
/gcta --bfile /chr${chr} --ld-score-region 200 --ld-wind 1000 --threads 64 --out /chr${chr}
# Merge LD scores
cat *.score.ld > full_score.ld.temp
awk '!x[$0]++' full_score.ld.temp > allChr.score.ld
rm full_score.ld.temp

Step 2: LD/MAF Grouping
import pandas as pd
ldFilePath = "allChr.score.ld"
savePath   = "snp_info.csv"
# 2.1 Group SNPs by MAF
lds_seg = pd.read_csv(
  ldFilePath,
  delim_whitespace=True,
  usecols=["SNP", "ldscore_SNP", "freq"],
  dtype={"SNP": str, "ldscore_SNP": float, "freq": float}
)
maf1 = lds_seg[lds_seg["freq"] >= 0.1]
maf2 = lds_seg[(lds_seg["freq"] < 0.1) & (lds_seg["freq"] >= 0.01)]
maf3 = lds_seg[(lds_seg["freq"] < 0.01) & (lds_seg["freq"] >= 0.001)]
maf4 = lds_seg[lds_seg["freq"] < 0.001]
# 2.2 Split each MAF group by the median LD score
def split_ld(df):
  if df.empty:
     return pd.Series([], dtype=str), pd.Series([], dtype=str)
  median_ld = df["ldscore_SNP"].median()
  ld1 = df[df["ldscore_SNP"] <= median_ld]["SNP"]
  ld2 = df[df["ldscore_SNP"] > median_ld]["SNP"]
  return ld1, ld2
maf1_ld1, maf1_ld2 = split_ld(maf1)
maf2_ld1, maf2_ld2 = split_ld(maf2)
maf3_ld1, maf3_ld2 = split_ld(maf3)
maf4_ld1, maf4_ld2 = split_ld(maf4)
# 2.3 Merge all SNPs and remove duplicates
all_snps = pd.concat([
  maf1_ld1, maf1_ld2,
  maf2_ld1, maf2_ld2,
  maf3_ld1, maf3_ld2,
  maf4_ld1, maf4_ld2
]).drop_duplicates().reset_index(drop=True)
snp_info = pd.DataFrame({'SNP': all_snps})
# 2.4 Mark SNPs in their respective groups (each MAF and LD bin)
for col in ['maf1_ld1', 'maf1_ld2', 'maf2_ld1', 'maf2_ld2', 'maf3_ld1', 'maf3_ld2', 'maf4_ld1', 'maf4_ld2']:
    snp_info[col] = ''
def mark_snps(snp_list, col_name):
    snp_info.loc[snp_info['SNP'].isin(snp_list), col_name] = 1  
mark_snps(maf1_ld1, 'maf1_ld1')
mark_snps(maf1_ld2, 'maf1_ld2')
mark_snps(maf2_ld1, 'maf2_ld1')
mark_snps(maf2_ld2, 'maf2_ld2')
mark_snps(maf3_ld1, 'maf3_ld1')
mark_snps(maf3_ld2, 'maf3_ld2')
mark_snps(maf4_ld1, 'maf4_ld1')
mark_snps(maf4_ld2, 'maf4_ld2')
snp_info.to_csv(savePath, index=False)

Step 3: Construct GRM Matrix
#!/bin/bash
/mph \
--make_grm \
--bfile /allChr_merged \
--min_maf 0 \
--min_hwe_pval 1e-8 \
--snp_info /snp_info.csv \
--snp_weight maf${mafLevel}_ld${ldLevel} \
--num_threads 100 \
--out /maf${mafLevel}_ld${ldLevel}

Step 4: Relatedness Refinement
# Extract common variants (MAF > 0.01)
/plink2 \
--pfile /allChr_merged \
--maf 0.01 \
--make-bed \
--threads 8 \
--memory 240000 \
--out /allChr_merged_maf_gt_0.01
#Construct GCTA GRM for common variants
for i in {1..10};
do
/gcta \
--bfile /allChr_merged_maf_gt_0.01 \
--make-grm-alg 1 \
--make-grm-part 10 ${i} \
--threads 64 \
--out /common_part${i} \
;done
#Extract relatedness ID with genomic relationship coefficient above 0.05
/gcta \
--grm /common \
--grm-cutoff 0.05 \
--make-grm \
--threads 64 \
--out /unrelated_cutoff005
# Generate unrelated GRM
/mph \
--binary_grm_file /maf${mafLevel}_ld${ldLevel} \
--keep /unrelated_cutoff005_iid.txt \
--subset_grm \
--output_file /maf${mafLevel}_ld${ldLevel}

Step 5: REML Heritability 
# Estimate heritability using REML
/mph --grm_list /grm_list.txt --phenotype_file /${pheno}_pheno.csv --trait ${pheno} --covariate_file /${pheno}_cov.csv --reml --save_memory --num_threads 32 --output_file /${pheno}







